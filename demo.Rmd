---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r}
# Demo to experience the power of Data Transformation

# Clean Environment
rm(list=ls(all=T))

#Load required packages
library(dplyr)
library(caTools)
library(MASS)
library(rpart) # classification algorithm
library(C50) # classification algorithm
path <- 'E:/Workspace/Noor'

setwd(path)
getwd()

data <- read.csv("data.csv")

str(data)
summary(data)
sapply(data,function(x) sum(is.na(x)))
# no NA 's found

#converting target from factor to numeric to perform various statistics like correlation check, AUC check etc..,
data$target <- ifelse(data$target == 'f',0,1)
table(data$target)
#check correlation
cor(data)

#Per correlation , it clearly says that Year and Age is highly correlated, so remove one of them to avoid multicollinearity
#multicollinearity - (statistics) a case of multiple regression in which the predictor variables are themselves highly correlated
# Removing year from data set, altough height is 70% correlated with weight, we are keeping it to check if the model can improve, 
#else will remove in later point
data<- data[-1]

# Randomly split the data into training and testing sets
set.seed(1234)
split = sample.split(data$target, SplitRatio = 0.70)

# Split up the data using subset
train = subset(data, split==TRUE)
test = subset(data, split==FALSE)

table(train$target)
table(test$target)

colAUC(X = data,y = data$target,plotROC = TRUE)
#as column wise AUC tells us the AUC value is 60 being the highest for Height, Lets see the models AUC if it improves.

# #Running Logistic Regression
# model2glm <- glm(target~. , data=train, family = 'binomial')
# 
# stepAIC(model2glm,direction="backward")
# # giving all attributeds to the model gives better AIC value than the Step advised one
# 
# model2glm <- glm(target~ age + height + weight, data=train, family = 'binomial')
# 
# PredictRaw <- predict(object = model2glm,
#                       newdata = train[!colnames(train)%in% 'target'],type = "response")
# pred_class <- ifelse(PredictRaw> 0.5, 'Y', 'N')
# 
# #Accuracy
# sum(diag(table(train$target,pred_class)))/sum(table(train$target,pred_class))# 83%
# 
# PredictRaw_tst <- predict(object = model2glm,
#                           newdata = test[!colnames(test)%in% 'target'],type = "response")
# pred_class_tst <- ifelse(PredictRaw_tst> 0.5, 'Y', 'N')
# 
# #Accuracy
# sum(diag(table(test$target,pred_class_tst)))/sum(table(test$target,pred_class_tst))

#########################################################################################
### Logistic Regression
#########################################################################################

LogReg <- glm(target ~ ., data=train, family=binomial)
summary(LogReg)
#
step(glm(target ~ .,data=train),direction = 'backward')
# step gave me age + height , But adding weight gave a slight improvement.
LogReg <- glm(formula = target ~ age + height + weight ,
              data = train)

# train results
prob<-predict(LogReg, type="response")
pred_class <- ifelse(prob> 0.5, 1, 0)
table(train$target,pred_class)

# Error Metric

conf.mat = table(train$target,pred_class)
cat("Accuracy : ",sum(diag(conf.mat))/sum(conf.mat))
cat("Recall : ",conf.mat[2,2]/sum(conf.mat[2,]))
cat("precision : ", conf.mat[2,2]/sum(conf.mat[,2]))
cat("F1 Score : ", 2*(conf.mat[2,2]/sum(conf.mat[,2])*conf.mat[2,2]/sum(conf.mat[2,]))/((conf.mat[2,2]/sum(conf.mat[,2])+conf.mat[2,2]/sum(conf.mat[2,]))))

# Test results
fitted.results <- predict(LogReg,test,type='response')
fitted.class <- ifelse(fitted.results > 0.6,1,0)
table(test$target,fitted.class)

# Error Metric
conf.mat = table(test$target,fitted.class)
cat("Accuracy : ",sum(diag(conf.mat))/sum(conf.mat))
cat("Recall : ",conf.mat[2,2]/sum(conf.mat[2,]))
cat("precision : ", conf.mat[2,2]/sum(conf.mat[,2]))
cat("F1 Score : ", 2*(conf.mat[2,2]/sum(conf.mat[,2])*conf.mat[2,2]/sum(conf.mat[2,]))/((conf.mat[2,2]/sum(conf.mat[,2])+conf.mat[2,2]/sum(conf.mat[2,]))))

#Ploting the ROC curve and calculate the AUC
#(area under the curve) which are typical performance measurements
#for a binary classifier.
#The ROC (Receiver Operating Characteristic curve) is a curve generated by plotting the true positive rate (TPR = sensitivity) against
# the false positive rate (FPR= specificity) at various threshold settings while the AUC is
# the area under the ROC curve. As a rule of thumb, a model with good
#predictive ability should have an AUC closer to 1 (1 is ideal) than to 0.5.

library(ROCR)
p <- predict(LogReg,test, type="response")
pr <- prediction(p, test$target)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf,colorize = TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))

abline(a=0, b= 1)

auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc 

#########################################################################################
### SVM
#########################################################################################
library(e1071)# classification algorithm
rm(list= ls()[!(ls() %in% c('train','test'))])
# Build best SVM model
PA_SVM <- svm(target ~ age + height + weight, data=train,  kernel = "polynomial")

# Look at the model summary
summary(PA_SVM)

plot(PA_SVM$index)

# Predict on train data
pred_Train  =  predict(PA_SVM, train)


plot(pred_Train) # Plot shows more than 0.5 

conf.mat = table(train$target, ifelse(pred_Train> 0.5, 1, 0))
conf.mat
cat("Accuracy : ",sum(diag(conf.mat))/sum(conf.mat))
cat("Recall : ",conf.mat[2,2]/sum(conf.mat[2,]))
cat("precision : ", conf.mat[2,2]/sum(conf.mat[,2]))
cat("F1 Score : ", 2*(conf.mat[2,2]/sum(conf.mat[,2])*conf.mat[2,2]/sum(conf.mat[2,]))/((conf.mat[2,2]/sum(conf.mat[,2])+conf.mat[2,2]/sum(conf.mat[2,]))))

# Predict on test data
pred_Test  =  predict(PA_SVM, test[setdiff(names(test),c('target'))])
conf.mat = table(test$target, ifelse(pred_Test> 0.5, 1, 0))
conf.mat
cat("Accuracy : ",sum(diag(conf.mat))/sum(conf.mat))
cat("Recall : ",conf.mat[2,2]/sum(conf.mat[2,]))
cat("precision : ", conf.mat[2,2]/sum(conf.mat[,2]))
cat("F1 Score : ", 2*(conf.mat[2,2]/sum(conf.mat[,2])*conf.mat[2,2]/sum(conf.mat[2,]))/((conf.mat[2,2]/sum(conf.mat[,2])+conf.mat[2,2]/sum(conf.mat[2,]))))

#########################################################################################
### Decision Tree
#########################################################################################

rm(list= ls()[!(ls() %in% c('train','test'))])

PA_rpart <- rpart(target~ ., data=train, method="class")
#By Variable Importance got below as top
PA_rpart <- rpart(target~age+height+weight, data=train, method="class")
plot(PA_rpart,main="Classification Tree for Gender Prediction",margin=0.15,uniform=TRUE)
text(PA_rpart,use.n=T)
summary(PA_rpart)

conf.mat = table(train$target, predict(PA_rpart, newdata=train, type="class"))

cat("Accuracy : ",sum(diag(conf.mat))/sum(conf.mat))
cat("Recall : ",conf.mat[2,2]/sum(conf.mat[2,]))
cat("precision : ", conf.mat[2,2]/sum(conf.mat[,2]))
cat("F1 Score : ", 2*(conf.mat[2,2]/sum(conf.mat[,2])*conf.mat[2,2]/sum(conf.mat[2,]))/((conf.mat[2,2]/sum(conf.mat[,2])+conf.mat[2,2]/sum(conf.mat[2,]))))

conf.mat = table(test$target, predict(PA_rpart, newdata=test, type="class"))

cat("Accuracy : ",sum(diag(conf.mat))/sum(conf.mat))
cat("Recall : ",conf.mat[2,2]/sum(conf.mat[2,]))
cat("precision : ", conf.mat[2,2]/sum(conf.mat[,2]))
cat("F1 Score : ", 2*(conf.mat[2,2]/sum(conf.mat[,2])*conf.mat[2,2]/sum(conf.mat[2,]))/((conf.mat[2,2]/sum(conf.mat[,2])+conf.mat[2,2]/sum(conf.mat[2,]))))

# # using c50
# 	
# dtC50 = C5.0(target ~ ., data = train, rules=TRUE)
# summary(dtC50)
# C5imp(dtC50, pct=TRUE)
# 
# conf.mat=table(train$loan, predict(dtC50, newdata=train, type="class"))
# rcTrain=(conf.mat[2,2])/(conf.mat[2,1]+conf.mat[2,2])*100
# a=table(test$loan, predict(dtC50, newdata=test, type="class"))
# rcTest=(a[2,2])/(a[2,1]+a[2,2])*100
# 
# cat("Accuracy : ",sum(diag(conf.mat))/sum(conf.mat))
# cat("Recall : ",conf.mat[2,2]/sum(conf.mat[2,]))
# cat("precision : ", conf.mat[2,2]/sum(conf.mat[,2]))
# cat("F1 Score : ", 2*(conf.mat[2,2]/sum(conf.mat[,2])*conf.mat[2,2]/sum(conf.mat[2,]))/((conf.mat[2,2]/sum
#                                                                                          
# cat("Recall in Training", rcTrain, '\n',
#     "Recall in Testing", rcTest
#     )
# 

#now lets do the transformation part to the same data ans see if that imporoves the model atall?
# let me do things fresh again

#Noor's Challange'

# Clean Environment

rm(list=ls(all=T))

#Load Data

data1 <- read.csv("E:/Workspace/Noor/data.csv")

str(data1)

summary(data1)
data1$target <- ifelse(data1$target == 'f', 0, 1)

cor(data1$year,data1$age)

#cor(data1$height,data1$male)



cor(data1)

table(data1$age)

#plot(data1$age,data1$male)

plot(data1$height,data1$weight)

install.packages("tcltk")
library(tcltk)
sqldf::sqldf('select count(*) from data1')

# combining height and age, as it gives the clear seperation view per the visualization that we have done in the step 1
data1$comb1 <- paste(data1$height
                  ,data1$age
                  ,sep = '.')

str(data1)

attributes <- setdiff(names(data1),c('target'))

# Fetching frequency rate for each attributes in the data set
select <- c('Select ')
case <- ', sum(case when target = 1 then 1 else 0 end) as True, sum(case when target = 1 then 1 else 0 end)+(sum(case when target = 0 then 1 else 0 end)) as Total from data1 group by '

for (i in 1:length(attributes)){

  ##Creating Frequency tables for all Individual attributes against the Class variable dynamically
  assign(attributes[i],sqldf::sqldf(paste(select,attributes[i],case,attributes[i],sep = '')))

}

# Calcuate response rate for each attribute

age              <-cbind(age[1],age[2]/age[3])
comb1              <-cbind(comb1[1],comb1[2]/comb1[3])
height                <-cbind(height[1],height[2]/height[3])
weight        <-cbind(weight[1],weight[2]/weight[3])
year           <-cbind(year[1],year[2]/year[3])

rm(select,case,i,attributes)

num_Data <- sqldf::sqldf('select a.True age, b.True comb, c.True height,d.True weight, e.True year , m.target
                            from data1 m
                            inner join age a on m.age = a.age
                            inner join comb1 b on m.comb1 = b.comb1
                            inner join height c on m.height = c.height
                            inner join weight d on m.weight = d.weight
                            inner join year e on m.year = e.year
                            ')

cor(num_Data)

#Removing unwanted/unused variables and relinquishing memory.
rm(list= ls()[!(ls() %in% c('num_Data','data1'))])

plot(num_Data$comb,num_Data$target)

str(num_Data)

# Randomly split the data into training and testing sets
set.seed(1234)
train_index = sample(x = nrow(num_Data),size = 0.7*nrow(num_Data))
train = num_Data[train_index,]
test = num_Data[-train_index,]


#########################################################################################
### Logistic Regression
#########################################################################################

LogReg <- glm(target ~ ., data=train, family=binomial)
summary(LogReg)
#
step(glm(target ~ .,data=train),direction = 'backward')
# step gave me NdcId + DoctorID + com3Id +Transdate, But removing Transdate gave a slight improvement.
LogReg <- glm(formula = target ~ comb + weight ,
              data = train)

# train results
prob<-predict(LogReg, type="response")
pred_class <- ifelse(prob> 0.5, 1, 0)
table(train$target,pred_class)

# Error Metric

conf.mat = table(train$target,pred_class)
cat("Accuracy : ",sum(diag(conf.mat))/sum(conf.mat))
cat("Recall : ",conf.mat[2,2]/sum(conf.mat[2,]))
cat("precision : ", conf.mat[2,2]/sum(conf.mat[,2]))
cat("F1 Score : ", 2*(conf.mat[2,2]/sum(conf.mat[,2])*conf.mat[2,2]/sum(conf.mat[2,]))/((conf.mat[2,2]/sum(conf.mat[,2])+conf.mat[2,2]/sum(conf.mat[2,]))))

# Test results
fitted.results <- predict(LogReg,test,type='response')
fitted.class <- ifelse(fitted.results > 0.6,1,0)
table(test$target,fitted.class)

# Error Metric
conf.mat = table(test$target,fitted.class)
cat("Accuracy : ",sum(diag(conf.mat))/sum(conf.mat))
cat("Recall : ",conf.mat[2,2]/sum(conf.mat[2,]))
cat("precision : ", conf.mat[2,2]/sum(conf.mat[,2]))
cat("F1 Score : ", 2*(conf.mat[2,2]/sum(conf.mat[,2])*conf.mat[2,2]/sum(conf.mat[2,]))/((conf.mat[2,2]/sum(conf.mat[,2])+conf.mat[2,2]/sum(conf.mat[2,]))))

#Ploting the ROC curve and calculate the AUC
#(area under the curve) which are typical performance measurements
#for a binary classifier.
#The ROC (Receiver Operating Characteristic curve) is a curve generated by plotting the true positive rate (TPR = sensitivity) against
# the false positive rate (FPR= specificity) at various threshold settings while the AUC is
# the area under the ROC curve. As a rule of thumb, a model with good
#predictive ability should have an AUC closer to 1 (1 is ideal) than to 0.5.

library(ROCR)
p <- predict(LogReg,test, type="response")
pr <- prediction(p, test$target)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf,colorize = TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))

abline(a=0, b= 1)

auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc 

#########################################################################################
### SVM
#########################################################################################
library(e1071)# classification algorithm
rm(list= ls()[!(ls() %in% c('train','test'))])
# Build best SVM model
PA_SVM <- svm(target ~ comb + weight, data=train,  kernel = "polynomial")

# Look at the model summary
summary(PA_SVM)

plot(PA_SVM$index)

# Predict on train data
pred_Train  =  predict(PA_SVM, train)


plot(pred_Train) # Plot shows more than 0.5 

conf.mat = table(train$target, ifelse(pred_Train> 0.5, 1, 0))
conf.mat
cat("Accuracy : ",sum(diag(conf.mat))/sum(conf.mat))
cat("Recall : ",conf.mat[2,2]/sum(conf.mat[2,]))
cat("precision : ", conf.mat[2,2]/sum(conf.mat[,2]))
cat("F1 Score : ", 2*(conf.mat[2,2]/sum(conf.mat[,2])*conf.mat[2,2]/sum(conf.mat[2,]))/((conf.mat[2,2]/sum(conf.mat[,2])+conf.mat[2,2]/sum(conf.mat[2,]))))

# Predict on test data
pred_Test  =  predict(PA_SVM, test[setdiff(names(test),c('target'))])
conf.mat = table(test$target, ifelse(pred_Test> 0.5, 1, 0))
conf.mat
cat("Accuracy : ",sum(diag(conf.mat))/sum(conf.mat))
cat("Recall : ",conf.mat[2,2]/sum(conf.mat[2,]))
cat("precision : ", conf.mat[2,2]/sum(conf.mat[,2]))
cat("F1 Score : ", 2*(conf.mat[2,2]/sum(conf.mat[,2])*conf.mat[2,2]/sum(conf.mat[2,]))/((conf.mat[2,2]/sum(conf.mat[,2])+conf.mat[2,2]/sum(conf.mat[2,]))))



```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).
